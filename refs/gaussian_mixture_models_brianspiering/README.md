A Gentle into Gaussian Mixture Models (GMM) & Expectation–Maximization (EM) Algorithm  
------

You know and love the Gaussian / Normal / Bell Curve. It is very common, appearing in almost all domains. In particular, it is the work-horse of statistics.

If one Gaussian distribution is awesome, what would be more awesome? 
TWO GAUSSIAN DISTRIBUTIONS!

That is what Gaussian Mixture Models (GMM) are - take your old friend the single Gaussian distribution and mix in another Gaussian distribution.

How does that dark magic happen? The Expectation–Maximization (EM) Algorithm.

This technical talk will start with a quick review of the Gaussian, then move in to GMMs, and discuss how to estimate a GMM with the EM algorithm. An introductory level of statistics is assumed. If you need a refresher, check out https://galvanizeopensource.github.io/stats-shortcourse/ or https://www.khanacademy.org/math/statistics-probability/modeling-distributions-of-data

----
__Suggested Preparation Materials__:

- Listen [EM on Talking Machines podcast](http://www.thetalkingmachines.com/blog/2015/10/9/machine-learning-mastery-and-cancer-clusters)
- Watch GMM & EM series, videos 16.3-16.13 from [mathematicalmonk](https://www.youtube.com/playlist?list=PLD0F06AA0D2E8FFBA8)
- "What is the expectation maximization algorithm?" pdf in readings folder

__Challenge Preparation Materials__:

- "EM Demystified- An Expectation-Maximization Tutorial" in readings folder
- [Theoretical Statistical approach to EM](http://www.stat.ucla.edu/~dinov/courses_students.dir/04/Spring/Stat233.dir/STAT233_notes.dir/EM_Tutorial)
